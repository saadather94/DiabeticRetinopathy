{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "5749c757-c8a9-48ad-95dd-fb69aebc3321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2, numpy as np, pathlib, os, PIL, PIL.Image, glob\n",
    "import numpy as np\n",
    "from skimage import data\n",
    "from skimage import io, morphology\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from PIL import Image\n",
    "import math\n",
    "import timeit\n",
    "import pywt\n",
    "from sklearn.metrics import accuracy_score as acc, auc, f1_score\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "\n",
    "width = 1024\n",
    "height = 1024\n",
    "dim = (width, height)\n",
    "sharpen = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17ab17-cfe7-4c6b-b352-578478fe2f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039e51d-b038-4afc-b837-b78787c26be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2cf35e1e-f5e5-4b66-86f6-7d14b5e94a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = np.array([[-5 ,3, 3],[-5,0,5],[-5,3,3]])\n",
    "m1 = np.array([[3 ,3, 3],[-5,0,3],[-5,-5,3]])\n",
    "m2 = np.array([[3 ,3, 3],[3,0,3],[-5,-5,-5]])\n",
    "m3 = np.array([[3 ,3, 3],[3,0,-5],[3,-5,-5]])\n",
    "m4 = np.array([[3 ,3,-5],[3,0,-5],[3,3,-5]])\n",
    "m5 = np.array([[3 ,-5, -5],[3,0,-5],[3,3,3]])\n",
    "m6 = np.array([[-5 ,-5, -5],[3,0,3],[3,3,3]])\n",
    "m7 = np.array([[-5 ,-5, 3],[-5,0,3],[3,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "febc8a79-d609-4f1c-8d14-f9a5213bc2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_size(img_size: int, kernel_size: int) -> int:\n",
    "    num_pixels = 0\n",
    "    \n",
    "    # From 0 up to img size (if img size = 224, then up to 223)\n",
    "    for i in range(img_size):\n",
    "        # Add the kernel size (let's say 3) to the current i\n",
    "        added = i + kernel_size\n",
    "        # It must be lower than the image size\n",
    "        if added <= img_size:\n",
    "            # Increment if so\n",
    "            num_pixels += 1\n",
    "            \n",
    "    return num_pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6a481e92-b979-4819-a0ab-6984e34a8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(img):\n",
    "    \n",
    "    _size = calculate_target_size(\n",
    "        img_size=img.shape[0],\n",
    "        kernel_size=3) #m0.shape[0]\n",
    "    \n",
    "    \n",
    "    k = m0.shape[0]\n",
    "    \n",
    "    # 2D array of zeros\n",
    "    convolved_img = np.zeros(shape=(width, height))\n",
    "    \n",
    "    # Iterate over the rows\n",
    "    for i in range(_size):\n",
    "        # Iterate over the columns\n",
    "        for j in range(_size):\n",
    "            # img[i, j] = individual pixel value\n",
    "            # Get the current matrix\n",
    "            \n",
    "            mat = img[i:i+k, j:j+k]\n",
    "            \n",
    "            # Apply the convolution - element-wise multiplication and summation of the result\n",
    "            # Store the result to i-th row and j-th column of our convolved_img array\n",
    "            \n",
    "            r0 = abs(np.sum(np.multiply(mat, m0)))\n",
    "            r1 = abs(np.sum(np.multiply(mat, m1)))\n",
    "            r2 = abs(np.sum(np.multiply(mat, m2)))\n",
    "            r3 = abs(np.sum(np.multiply(mat, m3)))\n",
    "            r4 = abs(np.sum(np.multiply(mat, m4)))\n",
    "            r5 = abs(np.sum(np.multiply(mat, m5)))\n",
    "            r6 = abs(np.sum(np.multiply(mat, m6)))\n",
    "            r7 = abs(np.sum(np.multiply(mat, m7)))\n",
    "\n",
    "            convolved_img[i, j] = max(r1,r2,r3,r4,r5,r6,r7)\n",
    "\n",
    "            \n",
    "    return convolved_img.astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "589f4a91-17ec-4a19-bc1b-b4642a257dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This function does the coefficient fusing according to the fusion method\n",
    "def fuseCoeff(cooef1, cooef2, method):\n",
    "\n",
    "    if (method == 'mean'):\n",
    "        cooef = (cooef1 + cooef2 ) / 3\n",
    "    # elif (method == 'min'):\n",
    "    #     cooef = np.minimum(cooef1,cooef2,cooef3)\n",
    "    # elif (method == 'max'):\n",
    "    #     cooef = np.maximum(cooef1,cooef2,cooef3)\n",
    "    # else:\n",
    "    #     cooef = []\n",
    "\n",
    "    return cooef\n",
    "\n",
    "\n",
    "def FUSION(I1,I2):\n",
    "\n",
    "    # Params\n",
    "    FUSION_METHOD = 'mean' # Can be 'min' || 'max || anything you choose according theory\n",
    "\n",
    "    # We need to have both images the same size\n",
    "    # I2 = cv2.resize(I2,I1.shape) # I do this just because i used two random images\n",
    "\n",
    "    ## Fusion algo\n",
    "\n",
    "    # First: Do wavelet transform on each image\n",
    "    wavelet = 'db1'\n",
    "    cooef1 = pywt.wavedec2(I1[:,:], wavelet)\n",
    "    cooef2 = pywt.wavedec2(I2[:,:], wavelet)\n",
    "    # cooef3 = pywt.wavedec2(I3[:,:], wavelet)\n",
    "\n",
    "    # Second: for each level in both image do the fusion according to the desire option\n",
    "    fusedCooef = []\n",
    "    for i in range(len(cooef1)-1):\n",
    "\n",
    "        # The first values in each decomposition is the apprximation values of the top level\n",
    "        if(i == 0):\n",
    "\n",
    "            fusedCooef.append(fuseCoeff(cooef1[0],cooef2[0],FUSION_METHOD))\n",
    "\n",
    "        else:\n",
    "\n",
    "            # For the rest of the levels we have tupels with 2 coeeficents\n",
    "            c1 = fuseCoeff(cooef1[i][0], cooef2[i][0],  FUSION_METHOD)\n",
    "            c2 = fuseCoeff(cooef1[i][1], cooef2[i][1],  FUSION_METHOD)\n",
    "            c3 = fuseCoeff(cooef1[i][2], cooef2[i][2], FUSION_METHOD)\n",
    "\n",
    "            # c3 = fuseCoeff(cooef1[i][2], cooef2[i][2], cooef3[i][2], FUSION_METHOD)\n",
    "\n",
    "            fusedCooef.append((c1,c2,c3))\n",
    "\n",
    "    # Third: After we fused the cooefficent we nned to transfor back to get the image\n",
    "    fusedImage = pywt.waverec2(fusedCooef, wavelet)\n",
    "\n",
    "    # Forth: normmalize values to be in uint8\n",
    "    fusedImage = np.multiply(np.divide(fusedImage - np.min(fusedImage),(np.max(fusedImage) - np.min(fusedImage))),255)\n",
    "    fusedImage = fusedImage.astype(np.uint8)\n",
    "\n",
    "    return fusedImage\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "82744529-6885-4cee-8830-a241ae22487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toimage(cleaned, mask):\n",
    "    r,c = cleaned.shape\n",
    "    cleaned = cleaned.astype('uint8')\n",
    "    for i in range(1,r):\n",
    "        for j in range(1,c):\n",
    "            if cleaned[i][j]==1:\n",
    "                cleaned[i][j]=255\n",
    "                \n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    cleaned = cv2.erode(cleaned,kernel,iterations = 1)\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_DILATE, kernel)\n",
    "    cleaned = cv2.bitwise_and(cleaned,cleaned,mask = mask)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463ac74-fcf0-42cf-af57-5fdd7371c311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "2633681d-821a-43d8-a652-b4004e5d9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PROCESSING(path):\n",
    "\n",
    "    img = cv.imread(path)\n",
    "    im1 = img[:,:,1]\n",
    "    im1 = cv2.resize(im1,dim, interpolation = cv2.INTER_AREA).reshape(width,height)\n",
    "\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))\n",
    "    enh = clahe.apply(im1)\n",
    "\n",
    "    ret,bin = cv2.threshold(im1,18,255,cv2.THRESH_BINARY)\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    mask = cv2.erode(bin,kernel,iterations = 1)\n",
    "\n",
    "\n",
    "\n",
    "    img = convolve(im1)\n",
    "\n",
    "    edg = cv2.filter2D(src=img, ddepth=-1, kernel=sharpen)\n",
    "    th, edg = cv2.threshold(edg, 120, 255, cv2.THRESH_BINARY )\n",
    "    edg = cv2.filter2D(src=edg, ddepth=-1, kernel=sharpen)\n",
    "    edg= edg>0\n",
    "    edg = morphology.remove_small_objects(edg, min_size=90, connectivity=20)\n",
    "    img_sharpened2 = toimage(edg,mask)\n",
    "\n",
    "\n",
    "    fusedimg = FUSION(img_sharpened2,enh)\n",
    "    \n",
    "    return fusedimg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733bac4-353d-46fe-a6f8-a364d5517427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab8b49-d118-47a6-9404-f0431eea800f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87184aec-9d56-4ed0-8545-7f21bd90d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('IMG').with_suffix('')\n",
    "\n",
    "NR = data_dir.glob('NR/*.*')\n",
    "AbNR = data_dir.glob('AbNR/*.*')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "n=1\n",
    "\n",
    "for i in NR:   \n",
    "    \n",
    "    s = str(n)\n",
    "    Path = r'C:\\Users\\RAPTOR\\Desktop\\Thesis\\CODE\\PROCESS_IMG\\BASE_PRO_IMG\\Normal\\NR_'+s+'.jpeg'\n",
    "    im = PROCESSING(str(i))\n",
    "    \n",
    "    cv2.imwrite(str(Path),im)\n",
    "    cv2.waitKey(0)\n",
    "    n+=1\n",
    "    data.append(im)\n",
    "    labels.append(0)\n",
    "    \n",
    "n=1\n",
    "    \n",
    "for i in AbNR:   \n",
    "    s = str(n)\n",
    "    Path = r'C:\\Users\\RAPTOR\\Desktop\\Thesis\\CODE\\PROCESS_IMG\\BASE_PRO_IMG\\AbNormal\\AbNR_'+s+'.jpeg'\n",
    "    im = PROCESSING(str(i))\n",
    "    \n",
    "    cv2.imwrite(str(Path),im)\n",
    "    cv2.waitKey(0)\n",
    "    n+=1\n",
    "    data.append(im)\n",
    "    labels.append(1)\n",
    "    \n",
    "\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "93896631-263d-4dc6-b3b2-a74986df1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(labels):\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = labels.reshape(len(labels), 1)           #before labels_batch\n",
    "    label_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return label_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a61b9-74a9-4a56-b871-209c37a8cd08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a0837f05-4824-4eb9-a6c5-c5d2bde34de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, ytrain, ytest = train_test_split(data, encode(labels), test_size=0.15,\n",
    "                                                random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd30f8-2818-483f-933d-0118670d23aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0bd2ca14-e8d3-4f87-88be-a3b40ef1e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BASE_MODEL\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 256, 256, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 252, 252, 16  416         ['input_4[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 252, 252, 16  64         ['conv2d_4[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 252, 252, 16  0           ['batch_normalization_3[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 126, 126, 32  4640        ['activation_3[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 126, 126, 32  128        ['conv2d_5[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 126, 126, 32  0           ['batch_normalization_4[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 126, 126, 32  9248        ['activation_4[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 126, 126, 32  128        ['conv2d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 126, 126, 32  544         ['activation_3[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 126, 126, 32  0           ['batch_normalization_5[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 126, 126, 32  0           ['conv2d_7[0][0]',               \n",
      "                                )                                 'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 63, 63, 32)  0           ['add_1[0][0]']                  \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 127008)       0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            254018      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 269,186\n",
      "Trainable params: 269,026\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model(width, height):\n",
    "\n",
    "    inputs = tf.keras.Input((width, height, 1))\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1)(inputs)\n",
    "    BN1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    relu1 = tf.keras.layers.Activation(activation='relu')(BN1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2,padding='same')(relu1)\n",
    "    BN2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    relu2 = tf.keras.layers.Activation(activation='relu')(BN2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,padding='same')(relu2)\n",
    "    BN3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    relu3 = tf.keras.layers.Activation(activation='relu')(BN3)\n",
    "    \n",
    "    skipconv = tf.keras.layers.Conv2D(filters=32, kernel_size=1, strides=2)(relu1)\n",
    "    \n",
    "    Add = tf.keras.layers.Add()([skipconv,relu3])\n",
    "    AvgPool = tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(Add)\n",
    "    FC = tf.keras.layers.Flatten()(AvgPool)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(2,activation='softmax')(FC)\n",
    "\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs, output,name='BASE_MODEL')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "width = data[0][0].shape[0]\n",
    "height = data[0][1].shape[0]\n",
    "model = get_model(width, height)\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "667c97ca-1ddb-4eb5-bc37-6f70fb52fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.1000 - acc: 0.9844 - precision_3: 0.9844 - recall_3: 0.9844\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0918 - acc: 0.9766 - precision_3: 0.9766 - recall_3: 0.9766\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0735 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0209 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0166 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000 - val_loss: 3.3182 - val_acc: 0.2667 - val_precision_3: 0.2667 - val_recall_3: 0.2667\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0299 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0361 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0258 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0137 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.0076 - acc: 1.0000 - precision_3: 1.0000 - recall_3: 1.0000 - val_loss: 4.1052 - val_acc: 0.2667 - val_precision_3: 0.2667 - val_recall_3: 0.2667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27e9c96ed30>"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('model.h5')\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['acc',tf.keras.metrics.Precision(),\n",
    "                      tf.keras.metrics.Recall()],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"image_classification.h5\", save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"acc\", patience=2)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "model.fit(\n",
    "    x=X_train,\n",
    "    y=ytrain,\n",
    "    validation_split=0.1,\n",
    "    validation_freq=5,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    \n",
    "    # callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e31fa-b9f5-4ae3-b3b9-ac230fc88290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9c764ce5-b2f3-4247-a27f-6cfad6537cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 509ms/step\n",
      "Model Classification report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72        13\n",
      "           1       1.00      0.23      0.38        13\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        26\n",
      "   macro avg       0.78      0.62      0.55        26\n",
      "weighted avg       0.78      0.62      0.55        26\n",
      " samples avg       0.62      0.62      0.62        26\n",
      "\n",
      "Model accuracy  61.53846153846154 %\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test).round()\n",
    "print('Model Classification report \\n\\n {}'.format(metrics.classification_report(ytest,pred)))\n",
    "print('Model accuracy  {} %'.format(metrics.accuracy_score(ytest,pred)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "99fb9c10-96e7-4638-bdfd-2d90b4e7d6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "class_names = ['NR','AbNR']  \n",
    "ax= plt.subplot()\n",
    "cm = metrics.confusion_matrix(ytest.argmax(axis=1), pred.argmax(axis=1))\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(class_names); ax.yaxis.set_ticklabels(class_names);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdbec9-24e8-4760-9e9c-be8dc76dd01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3df12-d599-4cf3-b425-81c3d423264e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e6069-9543-4e0f-b81e-a253a4b37a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27600437-5754-4600-afc0-0c4c482d8720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41b5a8-b575-4661-9b26-48c911158aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "19e4167e-bfb2-4c0b-9552-326a8f2ed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toimage(cleaned, mask):\n",
    "    r,c = cleaned.shape\n",
    "    cleaned = cleaned.astype('uint8')\n",
    "    for i in range(1,r):\n",
    "        for j in range(1,c):\n",
    "            if cleaned[i][j]==1:\n",
    "                cleaned[i][j]=255\n",
    "                \n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    cleaned = cv2.erode(cleaned,kernel,iterations = 1)\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_DILATE, kernel)\n",
    "    cleaned = cv2.bitwise_and(cleaned,cleaned,mask = mask)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def pre_process(img, NORMAL= True):\n",
    "    \n",
    "    # clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(3,3))\n",
    "    # im1 = clahe.apply(im1)\n",
    "\n",
    "    dst = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9, 2)\n",
    "    dst=cv2.threshold(dst, 100, 255, cv2.THRESH_OTSU )[1]\n",
    "    dst = cv2.medianBlur(dst,1)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(1,1))\n",
    "    dst = cv2.morphologyEx(dst, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "    dst = cv2.morphologyEx(dst, cv2.MORPH_CLOSE, kernel)\n",
    "    dst = 255 - dst\n",
    "\n",
    "    dst2 = cv2.morphologyEx(dst, cv2.MORPH_DILATE, kernel)\n",
    "\n",
    "\n",
    "    dst2 = dst > 0\n",
    "\n",
    "    if NORMAL:\n",
    "\n",
    "        cleaned = morphology.remove_small_objects(dst2, min_size=120) #For Normal = 120\n",
    "        cleaned = morphology.remove_small_holes(cleaned, 2)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        cleaned = morphology.remove_small_objects(dst2, min_size=40) #For Abnormal = 40\n",
    "        cleaned = morphology.remove_small_holes(cleaned, 2)\n",
    "        \n",
    "    return cleaned\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "25b40066-ce7c-48a4-bb20-15b5710ad063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PROCESSING(path):\n",
    "\n",
    "    img = cv.imread(path)\n",
    "    im1 = img[:,:,1]\n",
    "    im1 = cv2.resize(im1,dim, interpolation = cv2.INTER_AREA).reshape(width,height)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))\n",
    "    enh = clahe.apply(im1)\n",
    "\n",
    "    ret,bin = cv2.threshold(im1,18,255,cv2.THRESH_BINARY)\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    mask = cv2.erode(bin,kernel,iterations = 1)\n",
    "\n",
    "    img = pre_process(im1) \n",
    "    img = toimage(img,mask)\n",
    "\n",
    "    fusedimg = FUSION(img,enh)\n",
    "    \n",
    "    return fusedimg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e7251793-3c3a-435c-b509-6d6aead69b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('IMG').with_suffix('')\n",
    "\n",
    "NR = data_dir.glob('NR/*.*')\n",
    "AbNR = data_dir.glob('AbNR/*.*')\n",
    "\n",
    "data2 = []\n",
    "labels2 = []\n",
    "n=1\n",
    "\n",
    "for i in NR:   \n",
    "    \n",
    "    s = str(n)\n",
    "    Path = r'C:\\Users\\RAPTOR\\Desktop\\Thesis\\CODE\\PRO_IMG\\Normal\\NR_'+s+'.jpeg'\n",
    "    im = PROCESSING(str(i))\n",
    "    \n",
    "    cv2.imwrite(str(Path),im)\n",
    "    cv2.waitKey(0)\n",
    "    n+=1\n",
    "    data2.append(im)\n",
    "    labels2.append(0)\n",
    "    \n",
    "n=1\n",
    "    \n",
    "for i in AbNR:   \n",
    "    s = str(n)\n",
    "    Path = r'C:\\Users\\RAPTOR\\Desktop\\Thesis\\CODE\\PRO_IMG\\AbNormal\\AbNR_'+s+'.jpeg'\n",
    "    im = PROCESSING(str(i))\n",
    "    \n",
    "    cv2.imwrite(str(Path),im)\n",
    "    cv2.waitKey(0)\n",
    "    n+=1\n",
    "    data2.append(im)\n",
    "    labels2.append(1)\n",
    "    \n",
    "\n",
    "data2 = np.array(data2)\n",
    "labels2 = np.array(labels2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8692a-b824-43aa-8ecb-5501d50bb1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b9ac35d7-e564-4e01-8aed-27fdede126c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, ytrain, ytest = train_test_split(data2, encode(labels2), test_size=0.15,\n",
    "                                                random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ba8a10f9-104f-4777-b8af-58796d0b1cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BASE_MODEL\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 508, 508, 16  416         ['input_5[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 508, 508, 16  64         ['conv2d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 508, 508, 16  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 254, 254, 32  4640        ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 254, 254, 32  128        ['conv2d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 254, 254, 32  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 254, 254, 32  9248        ['activation_7[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 254, 254, 32  128        ['conv2d_10[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 254, 254, 32  544         ['activation_6[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 254, 254, 32  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 254, 254, 32  0           ['conv2d_11[0][0]',              \n",
      "                                )                                 'activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 127, 127, 32  0          ['add_2[0][0]']                  \n",
      " oling2D)                       )                                                                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 516128)       0           ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            1032258     ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,047,426\n",
      "Trainable params: 1,047,266\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model.\n",
    "width = data2[0][0].shape[0]\n",
    "height = data2[0][1].shape[0]\n",
    "model2 = get_model(width, height)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e382a281-67c8-4020-8dc7-073c0e379c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7274 - acc: 0.5703 - precision_5: 0.5703 - recall_5: 0.5703WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 646s 646s/step - loss: 0.7274 - acc: 0.5703 - precision_5: 0.5703 - recall_5: 0.5703\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.2159 - acc: 0.5938 - precision_5: 0.5938 - recall_5: 0.5938WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 675s 675s/step - loss: 5.2159 - acc: 0.5938 - precision_5: 0.5938 - recall_5: 0.5938\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8916 - acc: 0.6484 - precision_5: 0.6484 - recall_5: 0.6484WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 834s 834s/step - loss: 0.8916 - acc: 0.6484 - precision_5: 0.6484 - recall_5: 0.6484\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2472 - acc: 0.4844 - precision_5: 0.4844 - recall_5: 0.4844WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 11370s 11370s/step - loss: 2.2472 - acc: 0.4844 - precision_5: 0.4844 - recall_5: 0.4844\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 927s 927s/step - loss: 2.0617 - acc: 0.5625 - precision_5: 0.5625 - recall_5: 0.5625 - val_loss: 6.2122 - val_acc: 0.6000 - val_precision_5: 0.6000 - val_recall_5: 0.6000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5353 - acc: 0.8359 - precision_5: 0.8359 - recall_5: 0.8359WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 792s 792s/step - loss: 0.5353 - acc: 0.8359 - precision_5: 0.8359 - recall_5: 0.8359\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0685 - acc: 0.9844 - precision_5: 0.9844 - recall_5: 0.9844WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 842s 842s/step - loss: 0.0685 - acc: 0.9844 - precision_5: 0.9844 - recall_5: 0.9844\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4184 - acc: 0.8125 - precision_5: 0.8125 - recall_5: 0.8125WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 958s 958s/step - loss: 0.4184 - acc: 0.8125 - precision_5: 0.8125 - recall_5: 0.8125\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7382 - acc: 0.7344 - precision_5: 0.7344 - recall_5: 0.7344WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1/1 [==============================] - 730s 730s/step - loss: 0.7382 - acc: 0.7344 - precision_5: 0.7344 - recall_5: 0.7344\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 809s 809s/step - loss: 0.4203 - acc: 0.8047 - precision_5: 0.8047 - recall_5: 0.8047 - val_loss: 1.7837 - val_acc: 0.8667 - val_precision_5: 0.8667 - val_recall_5: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27e971a2df0>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('model.h5')\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model2.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=['acc',tf.keras.metrics.Precision(),\n",
    "                      tf.keras.metrics.Recall()],\n",
    ")\n",
    "\n",
    "# Define callbacks.\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"image_classification-2.h5\", save_best_only=True\n",
    ")\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"acc\", patience=4)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "model2.fit(\n",
    "    x=X_train,\n",
    "    y=ytrain,\n",
    "    validation_split=0.1,\n",
    "    validation_freq=5,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    batch_size=128,\n",
    "    \n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2180862-814d-42b3-8373-65a38d557a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "a5984ccb-5bbc-4a85-9359-c8e47bf1ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Model Classification report \n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.31      0.44        13\n",
      "           1       0.57      0.92      0.71        13\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        26\n",
      "   macro avg       0.69      0.62      0.58        26\n",
      "weighted avg       0.69      0.62      0.58        26\n",
      " samples avg       0.62      0.62      0.62        26\n",
      "\n",
      "Model accuracy  61.53846153846154 %\n"
     ]
    }
   ],
   "source": [
    "pred = model2.predict(X_test).round()\n",
    "print('Model Classification report \\n\\n {}'.format(metrics.classification_report(ytest,pred)))\n",
    "print('Model accuracy  {} %'.format(metrics.accuracy_score(ytest,pred)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6366722d-6453-4dad-8982-0aba57cdc289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "class_names = ['NR','AbNR']  \n",
    "ax= plt.subplot()\n",
    "cm = metrics.confusion_matrix(ytest.argmax(axis=1), pred.argmax(axis=1))\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(class_names); ax.yaxis.set_ticklabels(class_names);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f9721-3670-479a-ac6d-ca84518a9ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b54bd0-7b62-4f2b-84d4-23b80a771f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135abc3-16d8-4593-93b7-ae579cd8fa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccecc7f-daee-49a7-8603-cefd00da4564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6401bf7-887d-4162-8231-adabf3a5caea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0cbc1547-28c8-445e-aece-1638dd512932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BASE_MODEL\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 508, 508, 16  416         ['input_6[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 508, 508, 16  64         ['conv2d_12[0][0]']              \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 508, 508, 16  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 254, 254, 32  4640        ['activation_9[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 254, 254, 32  128        ['conv2d_13[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 254, 254, 32  0           ['batch_normalization_10[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 254, 254, 32  9248        ['activation_10[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 254, 254, 32  128        ['conv2d_14[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 254, 254, 32  544         ['activation_9[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 254, 254, 32  0           ['batch_normalization_11[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 254, 254, 32  0           ['conv2d_15[0][0]',              \n",
      "                                )                                 'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 127, 127, 32  0          ['add_3[0][0]']                  \n",
      " oling2D)                       )                                                                 \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 516128)       0           ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            1032258     ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,047,426\n",
      "Trainable params: 1,047,266\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model_1(width, height):\n",
    "\n",
    "    inputs = tf.keras.Input((width, height, 1))\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1)(inputs)\n",
    "    BN1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    relu1 = tf.keras.layers.Activation(activation='relu')(BN1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2,padding='same')(relu1)\n",
    "    BN2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    relu2 = tf.keras.layers.Activation(activation='relu')(BN2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,padding='same')(relu2)\n",
    "    BN3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    relu3 = tf.keras.layers.Activation(activation='relu')(BN3)\n",
    "    \n",
    "    skipconv = tf.keras.layers.Conv2D(filters=32, kernel_size=1, strides=2)(relu1)\n",
    "    \n",
    "    Add = tf.keras.layers.Add()([skipconv,relu3])\n",
    "    AvgPool = tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(Add)\n",
    "    FC = tf.keras.layers.Flatten()(AvgPool)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(2,activation='softmax')(FC)\n",
    "\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs, output,name='BASE_MODEL')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "# width = data[0][0].shape[0]\n",
    "# height = data[0][1].shape[0]\n",
    "model = get_model_1(width, height)\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dff319-8735-42da-beaa-b753996c5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_model_2(width, height):\n",
    "\n",
    "    inputs = tf.keras.Input((width, height, 1))\n",
    "    conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1)(inputs)\n",
    "    BN1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "    relu1 = tf.keras.layers.Activation(activation='relu')(BN1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2,padding='same')(relu1)\n",
    "    BN2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "    relu2 = tf.keras.layers.Activation(activation='relu')(BN2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,padding='same')(relu2)\n",
    "    BN3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "    relu3 = tf.keras.layers.Activation(activation='relu')(BN3)\n",
    "    \n",
    "    skipconv = tf.keras.layers.Conv2D(filters=32, kernel_size=1, strides=2)(relu1)\n",
    "    \n",
    "    Add = tf.keras.layers.Add()([skipconv,relu3])\n",
    "    AvgPool = tf.keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(Add)\n",
    "    FC = tf.keras.layers.Flatten()(AvgPool)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(2,activation='softmax')(FC)\n",
    "\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs, output,name='BASE_MODEL')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model.\n",
    "width = data[0][0].shape[0]\n",
    "height = data[0][1].shape[0]\n",
    "model = get_model(width, height)\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9562faa-24f2-4fc6-b66f-05f435978fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6474f-ab53-42cc-a7b0-1e7ba197258e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876f6da-94c2-47bb-8806-994c112d4958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cd204-53a0-495a-bf6d-988b7aac599a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d611ff-57b9-41a7-9431-cd426b141f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62c6bd-ac96-42a8-a869-1fed57cb746d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77f98f-535a-4193-8af8-8074c19be161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd48fbb-807e-4de8-b866-e26978dcc54e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
